# Tokenizer

## 分词
分词的任务，是把连续的文本序列切分成具有独立语义的基本单元（即“词”或“词元”）。

对于英文等天然有空格作为分隔符的语言，分词相对简单。
但对于中文、日文、泰文等语言，文本是连续的字符流，词之间没有明确的边界。例如，"给阿姨倒一杯卡布奇诺"，计算机需要依据算法将其正确地切分为 ["给", "阿姨", "倒", "一杯", "卡布奇诺"]。

## jieba分词（中文分词库）
它首先基于一个前缀词典（Trie树），高效地构建出一个包含句子中所有可能词语组合的有向无环图（DAG）。接着，通过动态规划算法寻找一条概率最大的路径，作为最终分词结果。
### 分词路径概率计算
该过程可量化为概率计算问题。假设分词路径由词语序列表示为 $w_1, w_2, \dots, w_n$，其中 $w_i$ 代表序列中第 $i$ 个词，则该路径的概率可近似为：

$$
P(w_1, w_2, \dots, w_n) \approx P(w_1) \times P(w_2) \times \dots \times P(w_n)
$$

其中，每个词 $w_i$ 的概率 $P(w_i)$ 可通过其在词典（语料库）中的频率估算，公式为：

$$
P(w_i) = \frac{\text{词 } w_i \text{ 的词频}}{\text{词典中所有词的总词频}}
$$

`jieba` 分词的核心目标，即找到使上述累乘概率值最大的分词路径。


`jieba`能够引入自定义词典，提升特定领域文本的分词效果。用户可以通过添加新词、调整词频等方式，优化分词结果。

>未登录词（OOV, Out-Of-Vocabulary）是一个“相对概念”——相对某个词典/词表未被收录的词。对词典法来说，就是词典里没有；
常见表现是被切成多个字或更小片段，需要通过维护词典（或引入其它模型）来修正。

## RNN向量表示（基于循环神经网络的语言模型）
RNN（循环神经网络）是一类适用于处理序列数据的神经网络架构。它通过在时间步之间共享参数，能够捕捉序列中的上下文信息。

### BiRNN 的结构与原理

BiRNN 的原理很简单：它由两个完全独立的 RNN 构成，并将它们叠加在一起处理信息。

（1）**一个正向 RNN**：按照从左到右的顺序读取输入序列（例如，从 $x_1$到 $x_T$），计算出一系列正向隐藏状态 $(\overrightarrow{h_1}, \overrightarrow{h_2}, ..., \overrightarrow{h_T})$。

（2）**一个反向 RNN**：按照从右到左的顺序读取输入序列（例如，从 $x_T$ 到 $x_1$），计算出一系列反向隐藏状态 $(\overleftarrow{h_1}, \overleftarrow{h_2}, ..., \overleftarrow{h_T})$。

在任意时间步 $t$，BiRNN 的最终输出 $h_t$ 是将该时间步对应的正向 RNN 隐藏状态 $\overrightarrow{h_t}$ 和反向 RNN 隐藏状态 $\overleftarrow{h_t}$ 进行拼接（Concatenate）得到的：

$$
h_t = [\overrightarrow{h_t} ; \overleftarrow{h_t}]
$$

通过这种方式， $h_t$ 就同时包含了输入序列中 $t$ 时刻左右两侧的上下文信息。需要注意，正向和反向的两个 RNN 拥有各自独立的权重参数，它们在训练过程中被 **同时优化**。

> **为什么不直接训练两个独立的 RNN 然后合并结果？**
>
> 当然可以分别训练一个正向 RNN 和一个反向 RNN，最后将它们的输出取平均（线性意见池）或几何平均（对数意见池）。但 BiRNN 的优势在于它是在同一个损失函数下**同时训练**两个方向的权重。所以正向和反向的特征提取是协同进行的，能够更好地适应目标任务，而不需要假设两个方向的预测是相互独立的。而且，BiRNN 彻底消除了对“时间延迟”参数 $M$ 的依赖，模型自动利用所有可用的过去和未来信息。

由于最终的输出是两个独立 RNN 隐藏状态的拼接，如果每个 RNN 的 `hidden_size` 都为 $H$，那么 BiRNN 在每个时间步的输出维度将变为 $2H$。

在 PyTorch 的 `nn.RNN` 模块中，只需将 `bidirectional` 参数设置为 `True` 即可轻松构建一个双向 RNN。其输出 `y` 的最后一个维度（特征维度）将是 `hidden_size` 的两倍，而最终隐藏状态 `h_n` 的第一个维度将是 `num_layers * 2`，分别存储了正向和反向 RNN 在最后一个时间步的隐藏状态。

## 随时间反向传播

前面已经了解了 RNN 如何通过正向传播逐个处理序列信息。接下来，将探讨其训练机制。RNN 的训练实质是标准反向传播（Backpropagation, BP）在时间展开图上的直接应用，称为**随时间反向传播（Backpropagation Through Time, BPTT）** [^3]。具体而言，该方法将 RNN 沿时间维度展开，可视作一个各层参数共享的深层前馈网络，进而在此结构上执行通用的反向传播算法。

假设整个序列的总损失 $L$ 是所有时间步损失 $L_t$ 的总和： $L = \sum_{t=1}^{T} L_t$。我们的目标是计算损失 $L$ 对共享参数 $U$ 和 $W$ 的梯度。

根据加法求导法则，总梯度等于每个时间步损失所贡献的梯度的总和：

$$
\frac{\partial L}{\partial W} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial W}
$$

主要的挑战在于计算单个时间步的梯度 $\frac{\partial L_t}{\partial W}$。由于 $W$ 在每个时间步都参与了计算，当前时刻的隐藏状态 $h_t$ 不仅依赖于当前输入 $x_t$，还通过 $h_{t-1}$ 间接依赖于之前所有时间步的输入和状态。因此，在 $t$ 时刻的损失，其梯度必须沿着时间反向传播，一直追溯到序列的开端。

使用链式法则（向量-导数矩阵形式）， $\frac{\partial L_t}{\partial W}$ 可分解为：

$$
\frac{\partial L_t}{\partial W} = \sum_{k=1}^{t} \underbrace{\frac{\partial L_t}{\partial h_t}}_{\text{梯度}} \cdot \underbrace{\frac{\partial h_t}{\partial h_k}}_{\text{导数矩阵连乘}} \cdot \underbrace{\frac{\partial h_k}{\partial W}}_{\text{直接影响}}
$$

其中， $\frac{\partial h_k}{\partial W}$ 是 $h_k$ 关于参数 $W$ 的导数矩阵； $\frac{\partial h_t}{\partial h_k}$ 表示从第 $k$ 步到第 $t$ 步的“传播的导数矩阵”，其本身是导数矩阵的连乘：

$$
\frac{\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}
$$

这个连乘的形式正是 RNN 产生问题的根源所在。

## RNN 的局限性

尽管 RNN 在理论上可以捕捉长距离依赖，但在实践中，基于 BPTT 的反向传播在时间维度上的导数矩阵连乘，带来了以下问题：

### 梯度消失与梯度爆炸

BPTT 链式求导的关键，在于梯度的反向传播路径上会形成一个连乘项 $\prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}$。以 $h_i = \tanh(U x_i + W h_{i-1})$ 为例，其传播的导数矩阵近似为：

$$
\frac{\partial h_i}{\partial h_{i-1}} \approx J_{\tanh}(U x_i + W h_{i-1}) \cdot W
$$

其中 $J_{\tanh}$ 表示 $\tanh$ 的导数矩阵。这意味着，从遥远的 $k$ 步到当前 $t$ 步的梯度传递，需要经历 $t-k$ 次与权重矩阵 $W$ 和激活函数导数矩阵的相乘。

1.  **梯度消失（Vanishing Gradients）**：在 RNN 中，权重矩阵 $W$ 的值通常会被初始化为较小的值。如果 $W$ 的范数（可以理解为其对向量的缩放能力）小于1，或者激活函数的导数（如 $\tanh'$ 最大为 1，通常小于 1）导致连乘项变小，那么多次相乘之后，梯度值会以指数级速度衰减，迅速趋近于0。当序列很长时（即 $t-k$ 很大），来自遥远过去的梯度信号在传播到当前步时，几乎完全消失。

2.  **梯度爆炸（Exploding Gradients）**：反之，如果 $W$ 的范数大于1，梯度值则会指数级增长，最终变成一个非常大的数值（Inf 或 NaN），导致模型训练崩溃。梯度爆炸问题相对容易发现和处理，一种常见的解决方法是**梯度裁剪（Gradient Clipping）**，即当梯度的范数超过某个阈值时，就将其缩放到该阈值。

### 长距离依赖

梯度消失在实践中比梯度爆炸更棘手，它直接导致了**长距离依赖问题（Long-term Dependency Problem）** [^4]。

-   **反向传播视角**：梯度消失意味着，模型无法学习到序列中相距遥远的词之间的依赖关系。正如 Werbos 所指出的，BPTT 本身是计算梯度的精确方法，但深层网络（或长序列）中的连乘效应是数学上的必然。例如，在句子 “孙悟空初到天庭时，被玉帝封为‘**弼马温**’，他嫌官小，心中不忿，便打出天门，返回花果山，自封‘齐天大圣’，后来又大闹天宫，搅乱了蟠桃盛会，偷吃了老君的金丹，最终被如来佛祖镇压在五行山下。五百年后，当有神仙旧事重提，用这个官职来称呼他时，依然是在**嘲讽**他。” 中，要正确理解结尾处“**嘲讽**”的含义，模型必须能关联到句子最开头的“**弼马温**”是一个低微官职这一事实。但由于两者之间间隔了极长的叙述，误差梯度在从“嘲讽”反向传播到“弼马温”时，可能已经衰减为零，导致模型无法捕捉到这种关键的远距离语义依赖。

-   **正向传播视角**：也可以理解为**信息遗忘**或**近期偏置**。在正向计算过程中，每一步的信息都会被新的输入和循环权重 $W$ “稀释”或“覆盖”。经过足够多的时间步后，序列最初的信息在隐藏状态中可能已所剩无几。这就好像两个句子，即使开头的词不同，在经过很长的相同后续序列后，它们最终的隐藏状态也可能变得几乎没有差别，模型“遗忘”了最初的差异。

### 单向性

常规 RNN 的信息流是单向的， $t$ 时刻的计算只能利用 $t$ 时刻之前的信息，无法利用未来的上下文信息。但在很多 NLP 任务中（如完形填空、机器翻译），一个词的含义同时取决于它的前文和后文。

> **双向循环神经网络** 能够通过结合正向和反向的信息流来解决单向性的问题。此外，为了学习更复杂的特征表示，还可以将多个 RNN 层堆叠起来，构成**深度循环神经网络**。然而，这些变体都未能从根本上解决梯度传播带来的长距离依赖问题。为了攻克这一核心难题，研究者们设计了两种更为精巧的门控 RNN 结构——**长短期记忆网络**和**门控循环单元**，下一节将对此进行详细探讨。